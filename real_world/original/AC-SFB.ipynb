{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spuriosity Didnâ€™t Kill the Classifier: Using Invariant Predictions to Harness Spurious Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def rademacher(beta, size):\n",
    "    \"\"\"Generate Rademacher random variables with probability `beta` for +1 and `1-beta` for -1.\"\"\"\n",
    "    return np.where(np.random.rand(size) < beta, 1, -1)\n",
    "\n",
    "def generate_dataset(beta_e, num_samples):\n",
    "    \"\"\"Generate dataset for given `beta_e` and number of samples.\"\"\"\n",
    "    # Generate Y ~ Rad(0.5)\n",
    "    Y = rademacher(0.5, num_samples)\n",
    "    \n",
    "    # Generate X_S = Y * Rad(0.75)\n",
    "    X_S = Y * rademacher(0.75, num_samples)\n",
    "    \n",
    "    # Generate X_U = Y * Rad(beta_e)\n",
    "    X_U = Y * rademacher(beta_e, num_samples)\n",
    "    \n",
    "    # Combine X_S and X_U into X\n",
    "    X = np.stack((X_S, X_U), axis=1)\n",
    "\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    Y_tensor = torch.tensor(Y, dtype=torch.float32).unsqueeze(1)\n",
    "    Y_tensor = (Y_tensor + 1) / 2\n",
    "    return X_tensor, Y_tensor\n",
    "\n",
    "# Parameters\n",
    "num_samples = 1000  # Number of samples per domain\n",
    "train_betas = [0.95, 0.7]\n",
    "val_beta = 0.6\n",
    "test_beta = 0.1\n",
    "batch_size = 32\n",
    "\n",
    "# Generate datasets\n",
    "train_domains = [generate_dataset(beta, num_samples) for beta in train_betas]\n",
    "val_domain = generate_dataset(val_beta, num_samples)\n",
    "test_domain = generate_dataset(test_beta, num_samples)\n",
    "\n",
    "# Concatenate the training data from different domains\n",
    "X_train = torch.cat([X for X, _ in train_domains], dim=0)\n",
    "Y_train = torch.cat([Y for _, Y in train_domains], dim=0)\n",
    "\n",
    "train_datasets = [TensorDataset(X, Y) for X, Y in train_domains]\n",
    "val_dataset = TensorDataset(*val_domain)\n",
    "test_dataset = TensorDataset(*test_domain)\n",
    "\n",
    "train_loaders = [DataLoader(dataset, batch_size=batch_size, shuffle=True) for dataset in train_datasets]\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Create a combined training dataset and dataloader\n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_sample:\n",
      " torch.Size([32, 2])\n",
      "Y_sample:\n",
      " torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "# Get an iterator from the DataLoader\n",
    "data_iter = iter(train_loader)\n",
    "\n",
    "# Get the first batch (or sample) from the iterator\n",
    "X_sample, Y_sample = next(data_iter)\n",
    "\n",
    "# Print the sample\n",
    "print(\"X_sample:\\n\", X_sample.shape)\n",
    "print(\"Y_sample:\\n\", Y_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Validation Loss: 0.7090\n",
      "Epoch [2/50], Validation Loss: 0.7038\n",
      "Epoch [3/50], Validation Loss: 0.6988\n",
      "Epoch [4/50], Validation Loss: 0.6939\n",
      "Epoch [5/50], Validation Loss: 0.6892\n",
      "Epoch [6/50], Validation Loss: 0.6847\n",
      "Epoch [7/50], Validation Loss: 0.6802\n",
      "Epoch [8/50], Validation Loss: 0.6755\n",
      "Epoch [9/50], Validation Loss: 0.6710\n",
      "Epoch [10/50], Validation Loss: 0.6667\n",
      "Epoch [11/50], Validation Loss: 0.6624\n",
      "Epoch [12/50], Validation Loss: 0.6580\n",
      "Epoch [13/50], Validation Loss: 0.6538\n",
      "Epoch [14/50], Validation Loss: 0.6493\n",
      "Epoch [15/50], Validation Loss: 0.6452\n",
      "Epoch [16/50], Validation Loss: 0.6409\n",
      "Epoch [17/50], Validation Loss: 0.6366\n",
      "Epoch [18/50], Validation Loss: 0.6325\n",
      "Epoch [19/50], Validation Loss: 0.6285\n",
      "Epoch [20/50], Validation Loss: 0.6245\n",
      "Epoch [21/50], Validation Loss: 0.6207\n",
      "Epoch [22/50], Validation Loss: 0.6171\n",
      "Epoch [23/50], Validation Loss: 0.6136\n",
      "Epoch [24/50], Validation Loss: 0.6104\n",
      "Epoch [25/50], Validation Loss: 0.6071\n",
      "Epoch [26/50], Validation Loss: 0.6043\n",
      "Epoch [27/50], Validation Loss: 0.6015\n",
      "Epoch [28/50], Validation Loss: 0.5991\n",
      "Epoch [29/50], Validation Loss: 0.5969\n",
      "Epoch [30/50], Validation Loss: 0.5947\n",
      "Epoch [31/50], Validation Loss: 0.5930\n",
      "Epoch [32/50], Validation Loss: 0.5917\n",
      "Epoch [33/50], Validation Loss: 0.5904\n",
      "Epoch [34/50], Validation Loss: 0.5892\n",
      "Epoch [35/50], Validation Loss: 0.5883\n",
      "Epoch [36/50], Validation Loss: 0.5874\n",
      "Epoch [37/50], Validation Loss: 0.5866\n",
      "Epoch [38/50], Validation Loss: 0.5859\n",
      "Epoch [39/50], Validation Loss: 0.5853\n",
      "Epoch [40/50], Validation Loss: 0.5848\n",
      "Epoch [41/50], Validation Loss: 0.5843\n",
      "Epoch [42/50], Validation Loss: 0.5839\n",
      "Epoch [43/50], Validation Loss: 0.5836\n",
      "Epoch [44/50], Validation Loss: 0.5833\n",
      "Epoch [45/50], Validation Loss: 0.5830\n",
      "Epoch [46/50], Validation Loss: 0.5828\n",
      "Epoch [47/50], Validation Loss: 0.5827\n",
      "Epoch [48/50], Validation Loss: 0.5825\n",
      "Epoch [49/50], Validation Loss: 0.5824\n",
      "Epoch [50/50], Validation Loss: 0.5823\n",
      "Train Loss: 0.5625, Train Accuracy: 0.7530\n",
      "Test Loss: 0.5644, Test Accuracy: 0.7510\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the simple three-layer neural network model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size=2, hidden_size=8, output_size=1):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))  # Use sigmoid to output a probability\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleNN(input_size=1, hidden_size=8, output_size=1)\n",
    "criterion = nn.BCELoss()  # Binary cross-entropy loss for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, Y_batch in train_loader:\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        outputs = model(X_batch[:,0:1])  # Forward pass\n",
    "        loss = criterion(outputs, Y_batch)  # Compute loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, Y_batch in val_loader:\n",
    "            outputs = model(X_batch[:,0:1])\n",
    "            loss = criterion(outputs, Y_batch)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}')\n",
    "    \n",
    "    # Save the best model based on validation loss\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "\n",
    "# Load the best model for testing\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "\n",
    "train_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, Y_batch in train_loader:\n",
    "        outputs = model(X_batch[:,0:1])\n",
    "        loss = criterion(outputs, Y_batch)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Convert predictions to binary (0 or 1)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        correct += (predicted == Y_batch).sum().item()\n",
    "        total += Y_batch.size(0)\n",
    "\n",
    "train_loss /= len(train_loader)\n",
    "accuracy = correct / total\n",
    "print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {accuracy:.4f}')\n",
    "\n",
    "\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, Y_batch in test_loader:\n",
    "        outputs = model(X_batch[:,0:1])\n",
    "        loss = criterion(outputs, Y_batch)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Convert predictions to binary (0 or 1)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        correct += (predicted == Y_batch).sum().item()\n",
    "        total += Y_batch.size(0)\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "accuracy = correct / total\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Validation Loss: 0.7043\n",
      "Epoch [2/50], Validation Loss: 0.7053\n",
      "Epoch [3/50], Validation Loss: 0.7064\n",
      "Epoch [4/50], Validation Loss: 0.7074\n",
      "Epoch [5/50], Validation Loss: 0.7084\n",
      "Epoch [6/50], Validation Loss: 0.7095\n",
      "Epoch [7/50], Validation Loss: 0.7107\n",
      "Epoch [8/50], Validation Loss: 0.7118\n",
      "Epoch [9/50], Validation Loss: 0.7130\n",
      "Epoch [10/50], Validation Loss: 0.7143\n",
      "Epoch [11/50], Validation Loss: 0.7155\n",
      "Epoch [12/50], Validation Loss: 0.7168\n",
      "Epoch [13/50], Validation Loss: 0.7181\n",
      "Epoch [14/50], Validation Loss: 0.7195\n",
      "Epoch [15/50], Validation Loss: 0.7209\n",
      "Epoch [16/50], Validation Loss: 0.7223\n",
      "Epoch [17/50], Validation Loss: 0.7237\n",
      "Epoch [18/50], Validation Loss: 0.7251\n",
      "Epoch [19/50], Validation Loss: 0.7266\n",
      "Epoch [20/50], Validation Loss: 0.7281\n",
      "Epoch [21/50], Validation Loss: 0.7296\n",
      "Epoch [22/50], Validation Loss: 0.7311\n",
      "Epoch [23/50], Validation Loss: 0.7326\n",
      "Epoch [24/50], Validation Loss: 0.7342\n",
      "Epoch [25/50], Validation Loss: 0.7357\n",
      "Epoch [26/50], Validation Loss: 0.7372\n",
      "Epoch [27/50], Validation Loss: 0.7388\n",
      "Epoch [28/50], Validation Loss: 0.7403\n",
      "Epoch [29/50], Validation Loss: 0.7419\n",
      "Epoch [30/50], Validation Loss: 0.7434\n",
      "Epoch [31/50], Validation Loss: 0.7450\n",
      "Epoch [32/50], Validation Loss: 0.7465\n",
      "Epoch [33/50], Validation Loss: 0.7480\n",
      "Epoch [34/50], Validation Loss: 0.7495\n",
      "Epoch [35/50], Validation Loss: 0.7510\n",
      "Epoch [36/50], Validation Loss: 0.7525\n",
      "Epoch [37/50], Validation Loss: 0.7538\n",
      "Epoch [38/50], Validation Loss: 0.7551\n",
      "Epoch [39/50], Validation Loss: 0.7564\n",
      "Epoch [40/50], Validation Loss: 0.7577\n",
      "Epoch [41/50], Validation Loss: 0.7590\n",
      "Epoch [42/50], Validation Loss: 0.7602\n",
      "Epoch [43/50], Validation Loss: 0.7614\n",
      "Epoch [44/50], Validation Loss: 0.7626\n",
      "Epoch [45/50], Validation Loss: 0.7638\n",
      "Epoch [46/50], Validation Loss: 0.7649\n",
      "Epoch [47/50], Validation Loss: 0.7660\n",
      "Epoch [48/50], Validation Loss: 0.7672\n",
      "Epoch [49/50], Validation Loss: 0.7682\n",
      "Epoch [50/50], Validation Loss: 0.7693\n",
      "Train Loss: 0.8421, Train Accuracy: 0.3235\n",
      "Test Loss: 0.6199, Test Accuracy: 0.7020\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "modelU = SimpleNN(input_size=1, hidden_size=8, output_size=1)\n",
    "criterionU = nn.BCELoss()  # Binary cross-entropy loss for binary classification\n",
    "optimizerU = optim.Adam(modelU.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    modelU.train()\n",
    "    model.eval()\n",
    "    for X_batch, _ in test_loader:\n",
    "        optimizerU.zero_grad()  # Clear gradients\n",
    "        Y_batch = model(X_batch[:,0:1])  # Forward pass\n",
    "        Y_batch = (Y_batch > 0.5).float()\n",
    "        outputs = modelU(X_batch[:,1:2])  # Forward pass\n",
    "        lossU = criterionU(outputs, Y_batch)  # Compute loss\n",
    "        lossU.backward()  # Backward pass\n",
    "        optimizerU.step()  # Update weights\n",
    "    \n",
    "    # Validation\n",
    "    modelU.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, Y_batch in val_loader:\n",
    "            Y_batch = model(X_batch[:,0:1])  # Forward pass\n",
    "            Y_batch = (Y_batch > 0.5).float()\n",
    "            outputs = modelU(X_batch[:,1:2])  # Forward pass\n",
    "            loss = criterion(outputs, Y_batch)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}')\n",
    "    \n",
    "    # Save the best model based on validation loss\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_stateU = modelU.state_dict()\n",
    "    best_model_stateU = modelU.state_dict()\n",
    "\n",
    "# Load the best model for testing\n",
    "modelU.load_state_dict(best_model_stateU)\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "modelU.eval()\n",
    "\n",
    "\n",
    "train_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, _ in train_loader:\n",
    "        Y_batch = model(X_batch[:,0:1])  # Forward pass\n",
    "        Y_batch = (Y_batch > 0.5).float()\n",
    "        outputs = modelU(X_batch[:,1:2])  # Forward pass\n",
    "        loss = criterion(outputs, Y_batch)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Convert predictions to binary (0 or 1)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        correct += (predicted == Y_batch).sum().item()\n",
    "        total += Y_batch.size(0)\n",
    "\n",
    "train_loss /= len(train_loader)\n",
    "accuracy = correct / total\n",
    "print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {accuracy:.4f}')\n",
    "\n",
    "\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, _ in test_loader:\n",
    "        Y_batch = model(X_batch[:,0:1])  # Forward pass\n",
    "        Y_batch = (Y_batch > 0.5).float()\n",
    "        outputs = modelU(X_batch[:,1:2])  # Forward pass\n",
    "        loss = criterion(outputs, Y_batch)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Convert predictions to binary (0 or 1)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        correct += (predicted == Y_batch).sum().item()\n",
    "        total += Y_batch.size(0)\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "accuracy = correct / total\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Accuracy: 0.8325\n",
      "test Accuracy: 0.8890\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(best_model_state)\n",
    "model.eval()\n",
    "modelU.load_state_dict(best_model_stateU)\n",
    "modelU.eval()\n",
    "\n",
    "def f(which_loader, name):\n",
    "    PY = 0\n",
    "    n1 = 0\n",
    "    n  = 0\n",
    "    e0 = 0\n",
    "    e1 = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, Y_batch in which_loader:\n",
    "            PY += Y_batch.sum().item()\n",
    "            Y_batch = model(X_batch[:,0:1])  # Forward pass\n",
    "            Y_batch = (Y_batch > 0.5).float()\n",
    "            outputs = modelU(X_batch[:,1:2])  # Forward pass\n",
    "\n",
    "            e0 += ((1-Y_batch)*(1-outputs)).sum().item()\n",
    "            e1 += (Y_batch*outputs).sum().item()\n",
    "\n",
    "            n1 += Y_batch.sum().item()\n",
    "            n += Y_batch.size(0)\n",
    "    e0 = e0 / (n-n1)\n",
    "    e1 = e1 / n1\n",
    "    PY = PY / n\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    OOD = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, Y_batch in which_loader:\n",
    "            YY_batch = model(X_batch[:,0:1])  # Forward pass\n",
    "            Xlogit = torch.logit(YY_batch, eps=1e-6)\n",
    "            YY_batch = (YY_batch > 0.5).float()\n",
    "            outputs = modelU(X_batch[:,1:2])  # Forward pass\n",
    "\n",
    "            outputs = (outputs + e0 - 1) / (e1 + e0 - 1)\n",
    "            outputs = torch.clamp(outputs, min=0, max=1)\n",
    "            Ulogit = torch.logit(outputs, eps=1e-6)\n",
    "\n",
    "            predict = torch.sigmoid(Xlogit + Ulogit - np.log(PY / (1 - PY)))\n",
    "\n",
    "            # Convert predictions to binary (0 or 1)\n",
    "            predicted = (predict > 0.5).float()\n",
    "            OOD += predicted.sum().item()\n",
    "            correct += (predicted == Y_batch).sum().item()\n",
    "            total += Y_batch.size(0)\n",
    "    OOD = OOD / total\n",
    "    accuracy = correct / total\n",
    "    print(name + f' Accuracy: {accuracy:.4f}')\n",
    "\n",
    "f(train_loader, 'train')\n",
    "f(test_loader, 'test')\n",
    "\n",
    "# PY, n1, n, e0, e1, torch.logit(outputs, eps=1e-6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
